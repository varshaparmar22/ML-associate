Date:- 30/05/2024

Amazon Services - AI/ML


@@@ Amazon Comprehend @@@

- NLP engine, gives you analytics for documents which you feed into it
- source can be any like email, social media input, web pages, medical records(medical comprehend)
- Does NLP stuff at high level

- Comprehend will extract
1. key phrases
2. entities
3. sentiments
4. language
5. syntax
6. topics
7. document classification

- can do event detection
- Personnel Information identification and redaction
- Targeted sentiment identification(for specific entity)
- can create your own data 

### Entities

Identify things like

- Organization
- Location
- Date
- Person

### Key Phrases

- Identify phrases from document
- Not saying about what they are
- saying about how important they are for given text 

### Language Detection

- Say language English or something with confidence score

### Sentiment Analysis

- identify sentiment of text like positive, negative, neutral, mix
- gives confidence score for each one

### Syntax

- Syntactical break-down of sentence
- Classifying things by Part of Speech(POS) they are
- like 
1. Proper noun
2. Punctuation
3. Auxilliary Verb
4. Verb 
5. Adposition

etc



@@@ Amzon Translate @@@

- take text as input
- Ofcourse for translation
- support some custom Terminology like 
	* In CSV or TMX format(standard format in MT)
	* like proper name or brand name that you want to translate in some form that you know about(custom)
	* pass in those format
- translate can work on streaming data(use transcribe to convert speech to text and translate)
- Gives confidence score

Amazon Translate uses a Translation Model that consists of two (2) components:
Encoder
Read a source text one word at a time.
Creates a semantic representation of each word.
Decoder
Uses the constructed semantic representation of the encoded text and translates it to the target language one word at a time.
The decoder also uses a method called the Attention mechanism to translate obscure words or phrases correctly.

When Automatic Language Detection is enabled, Amazon Translate uses Amazon Comprehend on the backend to automatically detect the language used in the source text.

@@@ Amazon Transcribe @@@

* Speech to Text
- takes input in format like FLAC, MP3, MP4, WAV
- Streaming audio is also supported from HTTP/2 or Web Socket
- Gives the text English, French, Spanish only 

* use cases
customer calls
Meeting transcription
captioning
Generating metadata to create a searchable archive

* Gives speaker identification 
- no of speaker and identify they are talking
- max 10 speaker

* Channel Identification
- Two callers could be transcribed separately
- two transcribe of text can be created out of that conversation
- Merging of those based on timing of utterance

* Automatic Language Identification
- you don't have to specify lang 
- It identify dominant one automatically

* Custom Vocabulary, Vocabulary Filtering
- Vocabulary Lists - like some special words : names, acronyms
- Vocabulary Tables - like "SoundsLike", "IPA"(international Phonetic Alphabet), "DisplayAs" <- it specify how to pronounce them, and how to transcribe them
- Allows you to create a list of words to filter from the transcript.

* Use Cases

$ Amazon Transcribe Call Analytics

- trained specifically for customer service and sales call
- it gives realtime transcription(transcribe) and insights of call
- identify sentiment, talk speed, interruption
- check for specific phrases like- "cancel my subscription"

$ Amazon Transcribe Medical

- Trained on medical Terminology
- HIPAA-eligible - takes patients privacy into account

$ Amazon Transcribe Subtitling

- provide live subtitle output from audio you feed in
- It adds timestamps to every word, making it easier to use for movie subtitles.

$ confidence score is added to result
$ lo-fi, hi-fi can handle


* concepts

A confidence score is between 0 and 100, indicating the probability that a given prediction is correct.
Low-fidelity (lo-fi) is a term used to describe audio recordings that exhibit poor sound quality. The term high-fidelity refers to high-quality audio recordings.
Automatic content redaction
	A process that censors sensitive information within the transcript output. 
	Replaces redacted information with the [PII] tag.


@@@ Amazon Polly @@@

- Text to Speech - remember parrot
- Neural 
- Many voices and languages it can create
- save into MP3, OGG, and PCM file formats.
- Offers Standard and Neural TTS (NTTS)

* Lexicon
- Customize pronunciation of some specific words and phrases
- example : W3C - World Wide Web Consortium <= acronym 
- you can tell polly to map W3C to World Wide Web Consortium

* SSML Format

- Speech Synthesis Markup Language
- along with plain text it gives you some more control
- like over emphasis, pronunciation, breathing, whispering, speech-rate, pitch, pause
- by this you can make resultant sound more lot natural

* Speech Mark

- you can get encode "when word/sentence start or ends" in audio stream
- helpful for lip-synching animation


@@@ Amazon Rekognition @@@

- its computer vision - easily build powerful applications to search, verify, and organize millions of image & extract motion-based context from stored or live stream videos and helps you analyze them.
- object and scene detection
- can use your own face collection
- image moderation - it will tell you if image has some offensive stuff init
- Facial Analysis - tell about male/female, how old, facial expression, emotion, wearing glasses
- Celebrity recognition
- face comparison
- text extracting in image
- video analysis : object, people, celebrity detection. people pathing - path that people follow when system start tracking people and identify facial landmarks like position of the left eye when person detected in video
- Rekognition Video allows you also to index metadata like objects, text, activities, scene, celebrities, and faces that make video search easy.
- DetectLabels API to detect label for object, scene, things etc with confidence score

## Inputs

- images from S3 bucket - faster way
- or can be pass as part of request
- facial recognition is depends on light, angle, visibility of eye, resolution
- video must from kinesis VS
	* H.264 encoded
	* 5-30 frames per seconds
	* resolution is more important than framerate - to get result
	* analyze videos (up to 8GB) stored in Amazon S3
	* formats are MPEG-4 and MOV.
- Lambda - to trigger image analysis

### Rekognition Custom Label

- train recognition system with some custom set of label - which can be found in images
- like custom label for team logo - like cricket team


@@@ Amazon Forecast @@@ 

- timeseries forecast service
- AutoML will choose best model for your time series data
	* DeepAR
	* ARIMA
	* ETS
	* NPTS
	* Prophet
- can work with wide variety of data like
	*Price
	*Promotion
	*Economic Performance
	*stock price

- you can combine related TS data to find relationship between them - like DeepAR 
- can be used for inventory, financial, resource planning(for example: for website how many server needed to handle traffic)
- its work based on dataset groups, predictors and forecasts


@@@ Amazon Forecast Algorithms @@@

1. CNN - QR - (Quantile Regression)

- best and computationally expensive
- work on large data set like hundreds of time series
- also accept the related historical time series data and metadata

2. DeepAR+

- Computationally expensive
- RNN
- Accept forward looking timeseries data and associated metadata

3. Prophet

- Additive model
- can handle non-linear trends and seasonality
- not expensive

4. Non-Parametric TimeSeries - NPTS

- simple and less expensive
- can work with sparse data - dataset having many missing series datapoints
- seasonality and climatological forecasts
- can combine them too like seasonal climatological forecast

5. ARIMA 

- Auto regressive integrated moving average
- dataset size is <100 data points than

6. ETS

- Exponential Smoothing
- appropriate for data set < 100 points


@@@ Amazon Lex @@@

- specify inner working of alexa
- NL - chatbot engine
- generate set of rules specify how chatbot will respond for certain kind of request
- Bot is built around intents
- intents to do some specific thing like "ordering pizza intent"
- say that utterance to invoke that intent
- code (lambda function) that will fulfill that intent
- for extra info - slots are there to fill up
- like pizza size, topping, crust type, when to deliver etc

$ you can deploy this chatbot on mobile SDK - aws(create mobile app for you), facebook messenger, slack or twillio

$ Amazon Lex Automated Chatbot Designer -automated process to build chatbot
- feed transcript of existing conversation 
- it will automatically apply NLP, deep learning on it 
- remove overlap and ambiguity
- so it can generate better model 
- it extract intents, slot values, user requests, phrases
- Ensure intents are separated from each other and well defined
- integrated with amazon connect transcript
	* amazon connect : contact center from cloud which helps superior customer service at low cost


@@@ Amazon Personalize @@@

- Fully managed recommendation engine - amazon use this
- feed the data by S3 or api or javascript snippet or SDK for offline
- user specific data will be sent here
- data should be in avro format : specify what data is what and correspondence to what
- two things it will do by api request
	1. GetRecommandations : recommand product or content / similar items
	2. GetPersonalizedRanking :  rank a list of items provided/ allows editors controls/curation about 
	   what is being recommended
- by console and CLI you can do it too 
- mainly api is used

### features
- real-time or batch recommendation
- recommendation for new user and new items - cold start problem - as system don't know anything about user
	* if new user then recommend popular items
	* for new items system will compute similarity every 2 hours so
	* if someone buy it, it will have relationship with other items and it wont stay new
- contextual recommendation
	* like device type or time based on that
- similar items
- unstructured text input : if sparse data for recommendation then system will use product description and similarity between items
- intelligent user segmentation : automatically classify users into groups - marketing campaign it can be used
- business rules and filters
	* filters : recently purchased items, cant be shipped or unavailable items
	* business rule : highlight the premium content like editors'pick
- Promoted Content
	* promoted content
	* can find relevant content too even it is promoted
- Trending Now
	* for given time-period most purchasing product by customers - summer holiday clothes
- Personalized Ranking
	* Search result
	* promotion
	* Curated List

### Amazon Personalized Terminology

$$ Datasets
- User, items, interactions - all related information about these each

$$ Recipes
- type of model can be build out of AP
- USER_PERSONALIZATION - 
- PERSONALIZED RANKING
- RELATED ITEMS

$$ Solutions
- train the model with data that you provide
- optimization for relevance and some additional objectives
	* like video length, price so people spend more time on watching long video or product which has high price
	* they will be optimized on that
	* additional objectives must be numeric
- HP optimization

$$ Campaign

- deploy your model's "solution version"
- deploy capacity for generating real-time recommendation like endpoint(servers) to generate
- for creating recommendation


$$ AP Hyper parameters


%% User Personalized / Personalized Ranking %%

- hidden_dimension : how many hidden parameters are there to optimize - specify 
- bptt : back propagation through time RNN,  time factor it will take into consideration
	- so some nearby time state will be focused more than so older one
- recency_mask : weight most recent event more
- min/max_user_history_length_percentile : throw out some percentage of user history which is out of pre-specified range
					   useful when some institution person order so many product from same Website
					   so he is not representing personnel interest
					   robots kind of entries like clicking on page link or rating set by robots
					   or crawlers will also do the same looking at everything
- exploration_weight : controls relevance - by default it is 0.3.
			higher rate will form less relevance in your result
			lower rate will form high relevance
- exploration_time_age_cut_off : how far back in time you want to consider for exploration, beyond some date(age)
				 throw away the data which are 5 years old so it is not for user's current interest

%% Similar Items %%

item_id_hidden_dimension : item id hidden dimension is automatically optimized for you
item_metadata_hidden_dimension : metadata provided for item by you - specify min/max range for it to work with


### maintain AP

- keep dataset current
- frequent import of data related to users
- PutEvents api call to feed real-time data for user
- re-train the model(incremental training)
	* its called new solution version
	* updates every 2 hours
	* should do full retain every week(trainingMode=Full)

### AP Security

- Data not shared across the account 
  * tempting to pool all users data together and get better results
  * as well as from many websites to pool data and get better recommendation
  * privacy concern is point to watch over there so not good thing to do
- Data will be encrypted with KMS
- Data may  be encrypted at rest in region specific by SSE-S3
- Data in-transit from your account to the server which will process it also encrypted by TLS-1.2
- Access control by IAM
- Data in s3 must have appropriate bucket policy to access by AP
- Monitoring and Logging by CloudWatch and CloudTrail

## Pricing
- Data ingestion : per GB
- Training : per hour
- Inference : per TPS-hours(Transaction per Second) more you hit for inference more it will charge - realtime
- Batch recommendation : not realtime, per user or per item


@@@ ML Services by AMAZON @@@

1. Amazon Textract
- its OCR which is smart can handles form, fields, and tables too from document
use cases:
Building search indexes
Importing documents into a business application
Building automated document processing solutions
Text extraction for Natural Language Processing (NLP) Applications
Maintaining document compliance

Amazon Textract returns a confidence score for each identified element, which indicates the probability that a given prediction is correct.

A low-confidence score can be rerouted to Amazon Augmented AI (A2I) for further human review.

The asynchronous operation allows you to process multipage PDF documents.

Detect Document Text API
	Uses optical character recognition (OCR) technology to extract printed text and handwriting from a document.
Analyze Document API
	Extracts printed text, handwriting, and other data from tables and key-value pairs from forms.

2. AWS DeepRacer
- Reinforcement learning powered 1/18 scale car racer
3. Industrial Application 
   * Amazon Lookout
   - for equipment : detect abnormality from senser data automatically to detect equipment issue
   - for metrics : monitors metrics from s3, rds, redshift and 3rd party SaaS app - out of order in metrics will alert anomalies
   - for vision : computer vision to detect defect, primarily for electronics like printing silicon wafers, circuit boards, microchips
                  take picture and find anomalies from that
4. Amazon Monitron :
	- End-to-End system for monitoring for industrial equipments 
	- for predictive maintenance
	- its kind of having sensor which take sensor and capture vibration or temperature data 	
	- has gateway to transfer that data to aws
	- monitron will analyse those data by ML and vibration standard
	- monitron app will setup hardware to receive alert on those abnormal equipment conditions
	

@@@ TorchServe and Neuron @@@

* TorchServe
- PyTorch project which is in collaboration between faceook and amazon
- TorchServe Is a framework which enable us to deploy pytorch model
- AWS resources can be used here to host hardware for pytorch project

* AWS Neuron
- AWS inferentia chip - microchip for deep learning
- EC2 instance Infrn1 - designed to have capability to get ML inference by using this chip
- aws neuron is just sdk for optimizing your model to work on inferentia chip
- integrated with SM, deep learning AMI's containers, TensorFlow, PyTorch, MXNet etc

* AWS Panorama
- gives computer vision at edge
- it collection of ML devices and a SDK(Software Development Kit) which brings computer vision to your internet protocol cameras stayed on premise
- makes prediction locally and with high accuracy
- you can analyse video in miliseconds
- route result to s3, cloudwatch, any business application for automation
- panorama enable is must either from aws or other devices
- system can be built around computer vision on the edge by using aws panorama

* AWS DeepComposer

- AI-powered keyboard
- create entire song if you feed one melody
- it use GAN network - Gen AI
- which compose of generator and discriminator NN 
- generating new content by learning to mimic the distribution of the training data. 

* Amazon Fraud Detector

- upload fraud data
- train the AFD
- it will create a model from template that you choose
- it gives api for your online application
- you send the transaction coming in and ask to AFD if its fraud or not
- keep incheck the risk associated to:
	- new accounts
	- guest checkout
	- try before you buy
	- online payments

* Amazon CodeGuru
- automated code review - ai powered
- any code from any GitHub, bitbucket, aws Codecommit feed in
- you can find out what is wrong with it
like
	- find lines that hurt your code performance
	- Resource leaks, race condition
	- fix security vulnerability automatically
	- offers specific recommendation to fix all stuff too
	- powered by ML
	- supports python and java programming

* Contact Lens for Amazon Connect

- service for customer support call center
- ingest recorded phone calls
- search for related call/chat
- can perform sentiment analysis - happy/upset/any - so service user can identify it
- find utterance that co-relate with successful calls - so service user can use them more often
- categorize call automatically
- measure talk speed and interruption - so service user can get feedback on his talk speed/interruption
- theme detection - new issue emerging - issue just came up that service user not know at all

* Amazon Kendra
- Enterprise search with natural language
- kind of internal tech support for people who are making some technical things
- combine data from shrepoint, internet, sharing services, JDBC, S3, in one searchable repository
- so enterprise search within data in you organization
- ML powered
- use thumbs up/down feedback from users and update data automatically
- relevance tunning - by view counts of doc visited by user
- Kendra is alexa's sister - corporate version of it

* Amazon Augmented AI(A2I)
- Human review for your ML predictions'
- build a workflow for reviewing low confidence prediction
- access to mechanical turk workforce or vendor associated with MTW to get human looking on your prediction
- integrated with textract and recognition
- integrated with sagemaker - so get human review for your any ML prediction made by SM
- very similar to ground truth - but do all in one plate

@@@  AWS DeepLens  @@@
AWS DeepLens is a cutting-edge, innovative device offered by Amazon Web Services (AWS) that combines the power of deep learning and Internet of Things (IoT) in a compact package. 
This smart video camera empowers developers with the ability to build and deploy custom computer vision models directly on the device itself.
wireless deep-learning enabled camera
Optimized for Apache MXNet, TensorFlow, and Caffe
Integrates with Amazon Rekognition for advanced image analysis
computing power (1 billion operations per second)

AWS DeepLens needs 3 AWS services to create a project:

Amazon SageMaker 
	Train/validate custom or pre-trained models 
AWS Lambda
	Preprocessing 
	Capturing inference
	Displaying output
AWS IoT Greengrass
	Deploys application project and Lambda runtime to AWS DeepLens 
	Handles software and configuration updates

AWS DeepLens Device Library
awscam module
	Runs inference code based on a projectâ€™s model.
mo module
	Converts Caffe, Apache MXNet, or TensorFlow deep-learning model artifacts into AWS DeepLens model artifacts.
	Provides optimizations for AWS DeepLens model artifacts.
DeepLens_Kinesis_Video module
	Can send video feeds from the AWS DeepLens device to Amazon Kinesis Video Streams.
